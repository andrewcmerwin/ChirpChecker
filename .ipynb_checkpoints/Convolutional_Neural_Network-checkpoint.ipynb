{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b167a869",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network \n",
    "There are two CNN models, 2d for matrix data(e.g. mfcc) and 1d for vector data (e.g. mfcc mean and variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b92115a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn import metrics\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras import layers # example functions: Dense, Dropout, Activation, Flatten, Conv1D, MaxPooling1D\n",
    "from keras import Input\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73bc46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run FeedingData.ipynb\n",
    "# read Calvin's function files so we can call those functions from this notebook\n",
    "# list of functions: \n",
    "# mfcc_(song, n=20)\n",
    "# mfcc_cut(song, n=20, cutoff = 800)\n",
    "# LFCC(song, n=20)\n",
    "# LFCC_no_low(song, n=20, cutoff = 800)\n",
    "# LFCC_no_low_clean(song, n=20, cutoff = 800)\n",
    "# mfcc_cut_cleaned(song, n=20, cutoff = 800)\n",
    "# main_freq(song, cutoff = 800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3169d39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hs_avg_var_mfccs(song, sr, n = 40, cutoff = 800):\n",
    "    max_ampj = np.argmax(abs(song))\n",
    "    time_start, time_stop = round(max_ampj - sr/4), round(max_ampj + sr/4)\n",
    "    if time_start < 0:\n",
    "        time_start, time_stop = 0, round(sr/2)\n",
    "    if time_stop > len(song):\n",
    "        time_start, time_stop = len(song) - round(sr/2), len(song)\n",
    "    mfccs = librosa.feature.mfcc(y = song[time_start:time_stop], fmin = cutoff, n_mfcc = n)\n",
    "    mfcc_avg = np.mean(mfccs, axis = 1)\n",
    "    mfcc_var = np.var(mfccs, axis = 1)\n",
    "    if time_start < 0: return (np.full(20, np.nan), np.full(20, np.nan))\n",
    "    else: \n",
    "        return (mfcc_avg, mfcc_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1af685f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hs_mfccs(song, sr, n = 40, cutoff = 800):\n",
    "    max_ampj = np.argmax(abs(song))\n",
    "    time_start, time_stop = round(max_ampj - sr/4), round(max_ampj + sr/4)\n",
    "    if time_start < 0:\n",
    "        time_start, time_stop = 0, round(sr/2)\n",
    "    if time_stop > len(song):\n",
    "        time_start, time_stop = len(song) - round(sr/2), len(song)\n",
    "    mfccs = librosa.feature.mfcc(y = song[time_start:time_stop], fmin = cutoff, n_mfcc = n)\n",
    "    return mfccs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88167334",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spectrogram(song, n_fft=512):\n",
    "    X = librosa.stft(song, n_fft=n_fft)\n",
    "    features=librosa.amplitude_to_db(abs(X))\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854aa6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vec_data(audio_path, meta_file, load_length):\n",
    "    songs_matrix = []\n",
    "    songs_id = []\n",
    "    df = pd.read_csv(meta_file)\n",
    "    id_list = df.cat_num\n",
    "\n",
    "    for name in id_list:\n",
    "        filepath = os.path.join(audio_path, str(name)+'.wav')\n",
    "        # skipping files that are less than 250 ~ 5sec long\n",
    "        # this is to make sure we have enough length for sampling fft time intervals\n",
    "        if os.path.getsize(filepath) < load_length*50000:\n",
    "            continue\n",
    "        # ============ load the data and get matrix ============\n",
    "        # make sure that the size of the feature matrix is the same for all files\n",
    "        song, sr = librosa.load(filepath, duration=load_length)\n",
    "        mean, var = get_hs_avg_var_mfccs(song, sr)\n",
    "        long_vec = np.concatenate((mean, var))\n",
    "        songs_matrix.append(long_vec)\n",
    "        songs_id.append(name)\n",
    "    return np.array(songs_matrix), songs_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01854753",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(audio_path, meta_file, load_length, get_feature_function, \n",
    "             n_mfcc=40, n_fft=512):\n",
    "    songs_matrix = []\n",
    "    songs_id = []\n",
    "    df = pd.read_csv(meta_file)\n",
    "    id_list = df.cat_num\n",
    "\n",
    "    for name in id_list:\n",
    "        filepath = os.path.join(audio_path, str(name)+'.wav')\n",
    "        try:\n",
    "            os.path.getsize(filepath)\n",
    "        except:\n",
    "            continue\n",
    "        # skipping files that are less than 250 ~ 5sec long\n",
    "        # this is to make sure we have enough length for sampling fft time intervals\n",
    "        if os.path.getsize(filepath) < load_length*50000:\n",
    "            continue\n",
    "        # ============ load the data and get matrix ============\n",
    "        # make sure that the size of the feature matrix is the same for all files\n",
    "        song, sr = librosa.load(filepath, duration=load_length)\n",
    "        if get_feature_function == get_hs_mfccs:\n",
    "            features = get_feature_function(song, sr)\n",
    "        elif get_feature_function == get_spectrogram:\n",
    "            features = get_feature_function(song, n_fft=512)\n",
    "        else:\n",
    "            features = get_feature_function(song, n = n_mfcc)\n",
    "        songs_matrix.append(features)\n",
    "        songs_id.append(name)\n",
    "    return np.array(songs_matrix), songs_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a82dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(numbers, meta_file, label_name):\n",
    "    # take in list of data id numbers and output their labels in label_name calssification\n",
    "    df = pd.read_csv(meta_file)\n",
    "    labels = []\n",
    "    for num in numbers:\n",
    "        labels.append(df[label_name].loc[df.cat_num == float(num)].item())\n",
    "    return np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5459874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for converting strings of label from one division to another, e.g. fam/subfam to critter name\n",
    "def convert_labels(labels, meta_file, from_division, to_division):\n",
    "    # find label in labels in meta_file under column from_division\n",
    "    # append as key in dictionary lookup_table, append as value the corresponding label in to_division\n",
    "\n",
    "    df = pd.read_csv(meta_file, usecols=[from_division, to_division])\n",
    "    lookup_table = dict() # look up table for new label\n",
    "    for label in labels:\n",
    "        if label in lookup_table.keys():\n",
    "            continue\n",
    "        idx = df.loc[df[from_division]==label].index[0]\n",
    "        lookup_table[label] = df[to_division][idx]\n",
    "    return [lookup_table[label] for label in labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6188e75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_small_class(songs_matrix, songs_label, threshold):\n",
    "    df_class = pd.DataFrame(np.unique(songs_label, return_counts=True)).T\n",
    "    print('Data read:\\n', df_class)\n",
    "    # remove classes with too few data points\n",
    "    small_class_list = []\n",
    "    for row in range(len(df_class)):\n",
    "        if df_class[1].iloc[row] < threshold:\n",
    "            small_class_list.append(df_class[0].iloc[row].item())\n",
    "    print('Removing classes:\\n',small_class_list)\n",
    "    delete_list = []\n",
    "    for small_class in small_class_list:\n",
    "        delete_list += list(np.where(songs_label==small_class)[0])\n",
    "\n",
    "    short_label = np.delete(songs_label, delete_list)\n",
    "    short_matrix = np.delete(songs_matrix, delete_list, 0)\n",
    "    # double check data after deleting\n",
    "    print('Reduced data:\\n', pd.DataFrame(np.unique(short_label, return_counts=True)).T)\n",
    "    return short_matrix, short_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f741ffb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    history_dict = history.history\n",
    "    epochs=len(list(history_dict.values())[0])\n",
    "    plt.figure(figsize = (4, 3))\n",
    "    plt.scatter(range(1,epochs+1), history_dict['accuracy'], label = \"Training Accuracy\")\n",
    "    plt.scatter(range(1,epochs+1), history_dict['val_accuracy'], label = \"Validation Set Accuracy\")\n",
    "    plt.xlabel(\"Epoch\", fontsize=12)\n",
    "    plt.ylabel(\"Accuracy\", fontsize=12)\n",
    "    plt.xticks(fontsize=10)\n",
    "    plt.yticks(fontsize=10)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize = (4, 3))\n",
    "    plt.scatter(range(1,epochs+1), history_dict['loss'], label = \"Training Loss\")\n",
    "    plt.scatter(range(1,epochs+1), history_dict['val_loss'], label = \"Validation Set Loss\")\n",
    "    plt.xlabel(\"Epoch\", fontsize=12)\n",
    "    plt.ylabel(\"Loss Function Value\", fontsize=12)\n",
    "    plt.xticks(fontsize=10)\n",
    "    plt.yticks(fontsize=10)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac5e497",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion(true_class, pred_class):\n",
    "    labels = (np.unique(true_class))\n",
    "    conf_mat = confusion_matrix(true_class, pred_class)#, labels=labels)\n",
    "    disp = ConfusionMatrixDisplay(conf_mat, display_labels=labels)\n",
    "    disp.plot(xticks_rotation='vertical')\n",
    "#    fig = disp.figure_\n",
    "#    fig.set_figwidth(3)\n",
    "#    fig.set_figheight(3) \n",
    "    plt.show()\n",
    "    accuracy = np.diagonal(conf_mat).sum()/np.sum(conf_mat)\n",
    "    print('accuracy =', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc069bd",
   "metadata": {},
   "source": [
    "### Create new data files from .wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe43836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all audio in audio_path\n",
    "# convert to matrix, save in np array, \n",
    "# load the corresponding label_name from lable_filename, save in another np array\n",
    "\n",
    "audio_path = 'E:\\chirpfiles\\\\selected_files'\n",
    "meta_file = 'MLNS_Final_Train.csv'\n",
    "\n",
    "start = time.time()\n",
    "max_time = 5 # cut off time for loading, in sec\n",
    "get_feature_function = mfcc_cut_cleaned\n",
    "# use get_data for matrix data and get_vec_data for vector data\n",
    "songs_matrix, songs_id = get_data(audio_path, meta_file, max_time, get_feature_function)\n",
    "songs_label_critter = get_labels(songs_id, meta_file, 'critter_name')\n",
    "songs_label_fam = get_labels(songs_id, meta_file, 'fam_or_subfam')\n",
    "end = time.time()\n",
    "print('time loading:', end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd62605f",
   "metadata": {},
   "source": [
    "### Save if created new data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c849a22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('E:\\chirpfiles\\\\songs_matrix_mfcc_halfsec.npy', songs_matrix)\n",
    "np.save('E:\\chirpfiles\\\\songs_label_mfcc_halfsec.npy', songs_label_critter)\n",
    "np.save('E:\\chirpfiles\\\\songs_label_mfcc_halfsec.npy', songs_label_fam)\n",
    "np.save('E:\\chirpfiles\\\\songs_id_mfcc_halfsec.npy', songs_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6516ac7",
   "metadata": {},
   "source": [
    "### Load if using existing data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bf3d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "songs_matrix = np.load('E:\\chirpfiles\\\\songs_matrix_mfcc_cut_cleaned_5sec.npy')\n",
    "songs_label_critter = np.load('E:\\chirpfiles\\\\songs_label_critter_5sec.npy')\n",
    "songs_label_fam = np.load('E:\\chirpfiles\\\\songs_label_fam_5sec.npy')\n",
    "songs_id = np.load('E:\\chirpfiles\\\\songs_id_5sec.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60d44b1",
   "metadata": {},
   "source": [
    "### If want to drop smaller classes for more balanced dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc3e893",
   "metadata": {},
   "outputs": [],
   "source": [
    "short_matrix, short_label = remove_small_class(songs_matrix, songs_label_critter, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fdaa58",
   "metadata": {},
   "source": [
    "### Choose the right label (fam or critter)\n",
    "used for unifying different version of matrix/label names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23b8298",
   "metadata": {},
   "outputs": [],
   "source": [
    "songs_matrix = short_matrix\n",
    "songs_label = short_label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4bc460",
   "metadata": {},
   "source": [
    "### 2D convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b745068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert insec fam into numbers. use le.inverse_transform to go from number to name\n",
    "le = LabelEncoder()\n",
    "songs_label_num = le.fit_transform(songs_label)\n",
    "songs_label_onehot = to_categorical(songs_label_num) # required output format by cnn\n",
    "\n",
    "# after 5/30, we are using already splitted data so this is already train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(songs_matrix, songs_label_onehot, stratify=songs_label_num,\n",
    "                                                  test_size = .2, random_state = 17)\n",
    "\n",
    "# Need two rounds of traintestsplit if the data is not already a training set\n",
    "#X_tt, X_val, y_tt, y_val = train_test_split(X_train, y_train, test_size = .2, random_state = 17)\n",
    "#print('Train validation split train shape: ', X_tt.shape)\n",
    "\n",
    "# make model \n",
    "input_shape =  (X_train.shape[1], X_train.shape[2],1,) # first dimension of train data is n_datapoints\n",
    "model = Sequential([\n",
    "    Input(input_shape),\n",
    "    layers.Conv2D(32, (3,3), padding='same', activation='relu'),\n",
    "    layers.MaxPooling2D((2,2), strides=2),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Conv2D(64, (3,3), padding='same', activation='relu'),\n",
    "    layers.MaxPooling2D((2,2), strides=2),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(len(le.classes_), activation='softmax')\n",
    "]) \n",
    "\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6469bae9",
   "metadata": {},
   "source": [
    "### 1D convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1467e37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert insec fam into numbers. use le.inverse_transform to go from number to name\n",
    "le = LabelEncoder()\n",
    "songs_label_num = le.fit_transform(songs_label)\n",
    "songs_label_onehot = to_categorical(songs_label_num) # required output format by cnn\n",
    "\n",
    "# rename data/label variable name so that we don't need to change them in the model block\n",
    "data_matrix = songs_matrix\n",
    "data_label = songs_label_onehot\n",
    "\n",
    "# after 5/30, we are using already splitted data so this is already train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(data_matrix, data_label, stratify=songs_label_num,\n",
    "                                                  test_size = .2, random_state = 17)\n",
    "\n",
    "#X_tt, X_val, y_tt, y_val = train_test_split(X_train, y_train, test_size = .2, random_state = 17)\n",
    "#print('Train validation split train shape: ', X_tt.shape)\n",
    "\n",
    "# make model \n",
    "input_shape = (X_train.shape[1],1) # first dimension of train data is n_datapoints\n",
    "model = Sequential([\n",
    "    Input(input_shape),\n",
    "    layers.Conv1D(32, 3, padding='same', activation='relu'),\n",
    "    layers.MaxPooling1D(2, strides=2),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Conv1D(64, 3, padding='same', activation='relu'),\n",
    "    layers.MaxPooling1D(2, strides=2),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Flatten(),\n",
    "    #layers.Dense(64, activation='relu'),\n",
    "    #layers.Dropout(0.2),\n",
    "    layers.Dense(len(le.classes_), activation='softmax')\n",
    "]) \n",
    "\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485239ed",
   "metadata": {},
   "source": [
    "### Train the model defined above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b706c9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "start = time.time()\n",
    "epochs = 50\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=4)\n",
    "checkpoint_filepath = 'E:/chirpfiles/ckpt/checkpoint.model.keras'\n",
    "model_checkpoint_callback = ModelCheckpoint(filepath=checkpoint_filepath, \n",
    "                                            monitor='val_accuracy',\n",
    "                                            mode='max',\n",
    "                                            save_best_only=True)\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=epochs, batch_size=64, \n",
    "                    validation_data=(X_val, y_val), \n",
    "                    callbacks=[early_stopping, model_checkpoint_callback])\n",
    "end = time.time()\n",
    "print('time training: ', end - start)\n",
    "model.load_weights(checkpoint_filepath) # load the best weight set for predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4db9931",
   "metadata": {},
   "source": [
    "### Save training history for plotting, just the accuracy and lost at each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1d4b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('E:\\chirpfiles\\hist_mfcc_c_c_twohiddenlayer_64_128_5sec_fam.pkl', 'wb') as f:  # Python 3: open(..., 'wb')\n",
    "    pickle.dump(history, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b9319e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830a8d3e",
   "metadata": {},
   "source": [
    "\n",
    "### Report accuracy on validation set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2031ab98",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predicted_vector = model.predict(X_val)\n",
    "predicted_class_index = np.argmax(predicted_vector, axis=-1)\n",
    "pred_class = le.inverse_transform(predicted_class_index)\n",
    "true_class_idx = np.where(y_val.astype(int))[1]\n",
    "true_class = le.inverse_transform(true_class_idx)\n",
    "plot_confusion(true_class, pred_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fc6d9d",
   "metadata": {},
   "source": [
    "## Forbidden test set block\n",
    "\n",
    "### load and transform data from test file, predict and plot confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74768373",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_path = 'E:\\chirpfiles\\\\no_voice_files'\n",
    "meta_file = 'MLNS_Final_Test.csv'\n",
    "\n",
    "test_songs_matrix, test_songs_id = get_data(audio_path, meta_file, max_time, get_feature_function)\n",
    "test_songs_label_critter = get_labels(test_songs_id, meta_file, 'critter_name')\n",
    "test_songs_label_fam = get_labels(test_songs_id, meta_file, 'fam_or_subfam')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d8ccc6",
   "metadata": {},
   "source": [
    "#### Save new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a376d7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('E:\\chirpfiles\\\\test_songs_matrix_mfcc_halfsec.npy', test_songs_matrix)\n",
    "np.save('E:\\chirpfiles\\\\test_songs_label_critter_mfcc_halfsec.npy', test_songs_label_critter)\n",
    "np.save('E:\\chirpfiles\\\\test_songs_label_fam_mfcc_halfsec.npy', test_songs_label_fam)\n",
    "np.save('E:\\chirpfiles\\\\test_songs_id_mfcc_halfsec.npy', test_songs_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a17816",
   "metadata": {},
   "source": [
    "#### Load existing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a25781",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_songs_matrix=np.load('E:\\chirpfiles\\\\test_songs_matrix_mfcc_cut_cleaned_5sec.npy')\n",
    "test_songs_label_critter=np.load('E:\\chirpfiles\\\\test_songs_label_critter_5sec.npy')\n",
    "test_songs_label_fam=np.load('E:\\chirpfiles\\\\test_songs_label_fam_5sec.npy')\n",
    "test_songs_id=np.load('E:\\chirpfiles\\\\test_songs_id_5sec.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6b8371",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts=np.unique(test_songs_label_fam, return_counts=True)[1]\n",
    "np.max(counts)/np.sum(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d76229",
   "metadata": {},
   "outputs": [],
   "source": [
    "short_matrix, short_label = remove_small_class(test_songs_matrix, test_songs_label_critter, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad1f1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_vector = model.predict(test_songs_matrix)\n",
    "predicted_class_index = np.argmax(predicted_vector, axis=-1)\n",
    "\n",
    "pred_class = le.inverse_transform(predicted_class_index)\n",
    "\n",
    "#true_class_idx = np.where(test_songs_label_fam.astype(int))[1]\n",
    "#true_class = le.inverse_transform(true_class_idx)\n",
    "true_class = test_songs_label_critter\n",
    "plot_confusion(true_class, pred_class,'critter')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea7358d",
   "metadata": {},
   "source": [
    "### If used family classification above, can reduce to critter using block below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a686362",
   "metadata": {},
   "outputs": [],
   "source": [
    "from_name = 'fam_or_subfam'\n",
    "to_name = 'critter_name'\n",
    "test_file = 'MLNS_Final_Test.csv'\n",
    "## to shorte list of classes after fitting on long list, use the class converter. \n",
    "true_class_short = convert_labels(true_class, test_file, from_name, to_name)\n",
    "pred_class_short = convert_labels(pred_class, test_file, from_name, to_name)\n",
    "\n",
    "plot_confusion(true_class_short, pred_class_short)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9484d94d",
   "metadata": {},
   "source": [
    "### Save prediction and true labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df5bbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('E:\\chirpfiles\\\\test_set_mfcc_c_c_twohiddenlayer_64_128_5sec_fam.pkl', 'wb') as f:  # Python 3: open(..., 'wb')\n",
    "    pickle.dump([true_class, pred_class], f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
