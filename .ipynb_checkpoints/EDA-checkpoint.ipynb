{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis and Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `librosa` package for audio processing and visualisation is used throughout our work. See https://librosa.org/doc/latest/index.html# for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa, librosa.display \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import scipy\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data we obtained from XXXX came in the form of 12602 many .wav files of varying lengths, with an accompanying metadata .csv file containing the following information:\n",
    "- Biological classification (genus, species).\n",
    "- Recording information (recordist, date, apparatus used, geographical location, etc.).\n",
    "\n",
    "To make the biological information more comprehensive, we added information on subfamilies(??) and families (??) to the metadata file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Add stuff from adding_fam_or_subfam.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Human Speech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of the files contained a human speech component at the beginning (the recordist or cataloguing scientist stating the index number of the recording, for instance). We used the following code to extract the second non-silent chunk of each audio file, which typically contained a useful section of the insect sound recording."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_Talking(f):\n",
    "    chirp_song, sr = librosa.load(f)\n",
    "    split=librosa.effects.split(chirp_song, top_db=60)\n",
    "    chirp_song_split=chirp_song[split[1,0]:split[1,1]]\n",
    "    if chirp_song_split.shape[0] > 220500:\n",
    "        chirp_song_split = chirp_song_split[:220500]\n",
    "    return chirp_song_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further EDA revealed that even after the initial human speech component was eliminated, some audio files still contained human speech. The following code located these audio files and removed them from our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLING_RATE = 16000\n",
    "\n",
    "import torch\n",
    "torch.set_num_threads(1)\n",
    "\n",
    "from IPython.display import Audio\n",
    "from pprint import pprint\n",
    "\n",
    "model, utils = torch.hub.load(repo_or_dir='snakers4/silero-vad',\n",
    "                              model='silero_vad',\n",
    "                              force_reload=True,\n",
    "                              onnx=False)\n",
    "\n",
    "(get_speech_timestamps,\n",
    " save_audio,\n",
    " read_audio,\n",
    " VADIterator,\n",
    " collect_chunks) = utils\n",
    "\n",
    "SAMPLING_RATE=16000\n",
    "wav = read_audio('E://Chirp_Files/117596.wav', sampling_rate=SAMPLING_RATE)\n",
    "# get speech timestamps from full audio file\n",
    "speech_timestamps = get_speech_timestamps(wav, model, sampling_rate=SAMPLING_RATE)\n",
    "pprint(speech_timestamps)\n",
    "\n",
    "import sys\n",
    "sys.path.append('E://Chirp_Files')\n",
    "\n",
    "import os\n",
    "# assign directory\n",
    "directory = 'E://Chirp_Files'\n",
    "no_voice_files=[]\n",
    "voice_files=[]\n",
    "\n",
    "# iterate over files in\n",
    "# that directory\n",
    "n = 0\n",
    "m = 0\n",
    "for filename in os.listdir(directory):\n",
    "    f = os.path.join(directory, filename)\n",
    "    # checking if it is a file\n",
    "    if os.path.isfile(f):\n",
    "        n+=1\n",
    "        m+=1\n",
    "        wav = read_audio(f, sampling_rate=SAMPLING_RATE)\n",
    "        speech_timestamps = get_speech_timestamps(wav, model, sampling_rate=SAMPLING_RATE)\n",
    "        if speech_timestamps == []:\n",
    "            no_voice_files.append(f)\n",
    "        else:\n",
    "            voice_files.append(f)\n",
    "        if n % 100 == 0:\n",
    "            print(m)\n",
    "            n = 0\n",
    "            df = pd.DataFrame(voice_files)\n",
    "            df.to_csv(\"voice_files.csv\", header=False, index=False)\n",
    "            df = pd.DataFrame(no_voice_files)\n",
    "            df.to_csv(\"no_voice_files.csv\", header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of the 12602 audio files remaining after `remove_Talking` was applied, this reduced the size of our data set to 6122. The final data set used in training and testing the model is `no_voice_files.csv`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Fill in librosa plotting code.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducing Background Noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step was to clean the audio of any further background noise. Since crickets, cicadas, and katydids typically chirp at frequencies in the 2-10kHz range, we used the following code to remove frequencies below 800Hz from the audio. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_low_freq(song, cutoff = 800):\n",
    "    D = np.abs(librosa.stft(song, n_fft=n_fft,  hop_length=hop_length))\n",
    "    cut = int(float(cutoff)/(librosa.fft_frequencies()[1]))\n",
    "    D = D[cut:]\n",
    "    return D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Fill in science-y stuff about MFCCs and LFCCs and why we use them.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code extracts MFCCs from the audio files used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_voice_list = pd.read_csv('no_voice_files.csv', header=None)\n",
    "filename_list = [name.split(\"\\\\\")[-1] for name in no_voice_list[0]]\n",
    "\n",
    "df = pd.read_csv('MLNS_Insects_Fams_05212024.csv')\n",
    "\n",
    "#This creates the new CSV with mfcc coefficients and f_min cuttoff = 800. Ignores files with length < .5 seconds. No need to run since the csv is now in the git!\n",
    "ct = 0\n",
    "n= 40\n",
    "for name in filename_list:\n",
    "    \n",
    "    #change this file path to the chirp_bucket_2 folder\n",
    "    file_path = '../../../chirp/'+name\n",
    "    song, sr = librosa.load(file_path)\n",
    "    if song.shape[0] > sr/2:\n",
    "        lib = librosa.feature.mfcc(y=song, sr=sr, fmin = 800, n_mfcc=n)\n",
    "        for i in range(0,n):\n",
    "            df.at[df.loc[df['cat_num']==int(name[:-4])].index[0], 'mfcc_'+str(i)+'_avg'] = lib[i].mean()\n",
    "            df.at[df.loc[df['cat_num']==int(name[:-4])].index[0], 'mfcc_'+str(i)+'_var'] = lib[i].var()\n",
    "           # df['mfcc_'+str(i)+'_avg']=lib[i].mean()\n",
    "           # df['mfcc_'+str(i)+'_var']=lib[i].var()\n",
    "        ct+=1\n",
    "        if ct % 100 == 0:\n",
    "            df.to_csv(\"MLNS_with_mfcc_stats_800_cutoff_05272024.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
