{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA) and Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data we obtained from XXXX came in the form of XXXX many .wav files of varying lengths, with an accompanying metadata .csv file containing the following information:\n",
    "- Biological classification (genus, species).\n",
    "- Recording information (recordist, date, apparatus used, geographical location, background species, etc.).\n",
    "(The `librosa` package for audio processing and visualisation is used throughout our work. See https://librosa.org/doc/latest/index.html# for more information.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa, librosa.display \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import scipy\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of the files contained a human speech component at the beginning (the recordist or scientist stating the catalogue number of the recording, for instance). We used the following code to extract the second non-silent chunk of each audio file, which typically contained a useful section of the insect sound recording."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_Talking(f):\n",
    "    chirp_song, sr = librosa.load(f)\n",
    "    split=librosa.effects.split(chirp_song, top_db=60)\n",
    "    chirp_song_split=chirp_song[split[1,0]:split[1,1]]\n",
    "    if chirp_song_split.shape[0] > 220500:\n",
    "        chirp_song_split = chirp_song_split[:220500]\n",
    "    return chirp_song_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further EDA revealed that even after the initial human speech component was eliminated, some audio files still contained human speech. The following code located these audio files and removed them from our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLING_RATE = 16000\n",
    "\n",
    "import torch\n",
    "torch.set_num_threads(1)\n",
    "\n",
    "from IPython.display import Audio\n",
    "from pprint import pprint\n",
    "\n",
    "model, utils = torch.hub.load(repo_or_dir='snakers4/silero-vad',\n",
    "                              model='silero_vad',\n",
    "                              force_reload=True,\n",
    "                              onnx=False)\n",
    "\n",
    "(get_speech_timestamps,\n",
    " save_audio,\n",
    " read_audio,\n",
    " VADIterator,\n",
    " collect_chunks) = utils\n",
    "\n",
    "SAMPLING_RATE=16000\n",
    "wav = read_audio('E://Chirp_Files/117596.wav', sampling_rate=SAMPLING_RATE)\n",
    "# get speech timestamps from full audio file\n",
    "speech_timestamps = get_speech_timestamps(wav, model, sampling_rate=SAMPLING_RATE)\n",
    "pprint(speech_timestamps)\n",
    "\n",
    "import sys\n",
    "sys.path.append('E://Chirp_Files')\n",
    "\n",
    "import os\n",
    "# assign directory\n",
    "directory = 'E://Chirp_Files'\n",
    "no_voice_files=[]\n",
    "voice_files=[]\n",
    "\n",
    "# iterate over files in\n",
    "# that directory\n",
    "n = 0\n",
    "m = 0\n",
    "for filename in os.listdir(directory):\n",
    "    f = os.path.join(directory, filename)\n",
    "    # checking if it is a file\n",
    "    if os.path.isfile(f):\n",
    "        n+=1\n",
    "        m+=1\n",
    "        wav = read_audio(f, sampling_rate=SAMPLING_RATE)\n",
    "        speech_timestamps = get_speech_timestamps(wav, model, sampling_rate=SAMPLING_RATE)\n",
    "        if speech_timestamps == []:\n",
    "            no_voice_files.append(f)\n",
    "        else:\n",
    "            voice_files.append(f)\n",
    "        if n % 100 == 0:\n",
    "            print(m)\n",
    "            n = 0\n",
    "            df = pd.DataFrame(voice_files)\n",
    "            df.to_csv(\"voice_files.csv\", header=False, index=False)\n",
    "            df = pd.DataFrame(no_voice_files)\n",
    "            df.to_csv(\"no_voice_files.csv\", header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of the 12602 audio files remaining after `remove_Talking` was applied, this reduced the size of our data set to 6122."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step was to clean the audio of any further background noise. Since crickets, cicadas, and katydids typically chirp at frequencies in the 2-10kHz range, we used the following code to remove frequencies below 800Hz from the audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_low_freq(song):\n",
    "    D = np.abs(librosa.stft(song, n_fft=n_fft,  hop_length=hop_length))\n",
    "    #FIND OUT HOW MANY FREQUENCIES WE REMOVED HERE!\n",
    "    D=D[64:]\n",
    "    return D"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
